前段时间写了这个小项目,目前实现的功能：（1）从15个市级文化管理部门和安徽省图书馆网站的指定栏目,自动抓取新闻消息的标题、链接、日期。（2）从指定信息数据文件，生成网页。（3）将生成的网页，经自己的微信，发给指定朋友或自己，这里也可以使用linux下的mail程序，发送到指定邮箱。

通过编写这个小项目，熟悉了Windows和Linux系统下Python编程以及一些实现细节，学习了如何分析网站数据。这个项目没有使用现成的爬虫框架，如scrapy，只是针对自己的需求，封装了一个非常简单的类。

为了减少程序中的重复代码，在编写过程中，经过几次重构，自己封装了一个类webmonkey.py，用于处理抓取网站时的一些共同操作，即打开网站，返回一个包括网站源码的响应对象，显示抓取到的信息等。然后针对每一个网站，在继承通用类webmonkey的基础上，仅改动网站url和网页信息提取规则，

关于总体设计。实际上，在这个项目一开始时，并没有非常明确的总体设计，现在看到目录结构，是在基本功能完成以后逐渐整理得来的，编写时随意性大了些，想到什么功能就写到哪里，如果发现不对，就对代码进行重构。

虽然没有什么明确应该是什么样的，但对项目的核心功能还是相对清楚的，就是从各个文化机构的网站抓取信息。具体实现时是，每个数据源对应一个独立的爬虫程序，然后在一个总的调度程序中循环调用。

关于数据保存。程序中没有实现数据保存的功能，而是利用操作系统提供的管道（“｜”）命令，将程序输出到本地文件进行保存。在数据量比较小的情况下，或用于演示时，用这种方法不大。如果考虑长期运行，肯定需要数据库。这个问题不是很大，自己在另外一个小项目中已经封装过一个sqlite3的类，需要时可以拿过来改改。

关于数据展示。为了展示抓取信息，写了一个网页生成的模块txt2html.py。基本的想法是先写一个网页模板，然后在这个模板的基础上，将抓取的信息写到网页文件中。另外，还写了一个模块，可以将HTML文件通过微信发送给自己。

现在还有几个问题没有解决。第一，抓取的信息较少.部分站点同一天会更新多条信息，目前只是抓取各网站相应栏目的第一条信息。第二个问题是接着第一个问题来的，数据量大了之后，用数据库文件可能更方便，所以数据保存部分要实现。

此外，数据栏目较窄的问题。信息页面设计的问题。未来有可能要实现的功能还有个性信息推荐功能。

如何跟网站结合起来？ 用一个程序，每天定时抓取数据，然后自动更新网页文件。
